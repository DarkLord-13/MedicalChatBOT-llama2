{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow:\n",
    "1. download llama2 model (the bloke on hf)\n",
    "2. load the model using ctransformers (we can't use transformers to load llama2 as we don't have gpu)\n",
    "   \n",
    "   python bindings for the transformers in c/c++\n",
    "3. sentence transformer -> embedding transformer to create the embeddings\n",
    "4. vector store -> to store the embeddings in lower dimensions\n",
    "\n",
    "   1. chromaDB\n",
    "   2. faissCPU\n",
    "   3. qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data -> Preprocessing (langchain) -> embedding using -> sentence transformer -> vector db/store (faissCPU) -> faissCPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user -> prompt (screen) -> faissCPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in vector store -> cosine similarity\n",
    "\n",
    "no latency\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # if u get error, replace it with sentence transformer\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "DB_FAISS_PATH = 'vectorstores/db_failures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition for chuck_size**\n",
    "\n",
    "GPT-3.5-turbo supports a context window of 4096 tokens â€” that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
    "\n",
    "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into gpt-3.5-turbo, leaving ~2000 tokens for conversation history and completion.\n",
    "\n",
    "With this ~2000 token limit we may want to include five snippets of relevant information, meaning each snippet can be no more than 400 token long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " loading PDF documents, extracting their text content, and splitting that content into smaller text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector DB\n",
    "def create_vector_db():\n",
    "    \n",
    "    # since our input documents are in pdf format, we use PyPDFLoader, and give extensions as .pdf in glob\n",
    "    # DATA_PATH specifies the path from where we will input our data\n",
    "    loader = DirectoryLoader(DATA_PATH, glob='*.pdf', loader_cls=PyPDFLoader) \n",
    "    documents = loader.load() # loaded documents are stored in the documents variable, as a list of objects\n",
    "    \n",
    "    # chunk_size -> specifies the number of tokens per list\n",
    "    # chunk_overlap -> how much overlap is taking place between start and end (sliding window concept)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                       model_kwargs = {'device':'cpu'})\n",
    "    \n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
